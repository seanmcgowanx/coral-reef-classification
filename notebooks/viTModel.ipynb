{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da131c5e",
   "metadata": {},
   "source": [
    "## Vision Transformer (google/vit-base-patch16-224-in21k)\n",
    "We chose google/vit-base-patch16-224-in21k over other ViT models because it provides the strongest balance of accuracy, stability, and computational efficiency for a project like ours: it is pretrained on the much larger ImageNet-21k dataset which gives it richer visual features than standard ImageNet-1k ViTs, it uses the well-established Base architecture which is powerful without being too heavy for a single-GPU Colab workflow, and it avoids the higher memory requirements and slower training times of larger ViT or Swin variants while still offering noticeably better performance on small to medium-sized image datasets like ours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961ff73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow imports from the src folder\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06508be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import ViTModel\n",
    "from tqdm import tqdm\n",
    "# Import custom functions\n",
    "from src.s3_loader import get_image_s3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff22b134",
   "metadata": {},
   "source": [
    "### Data Augmentations\n",
    "We apply transformations and augmentations so the model learns to recognize benthic features under many lighting conditions, angles, and image variations instead of memorizing a narrow set of appearances. This improves generalization by exposing the Vision Transformer to more realistic underwater variation, helping it perform better on new, unseen reef images.\n",
    "#### Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef7c60b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet statistics for normalization\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD  = (0.229, 0.224, 0.225)\n",
    "\n",
    "# Define data augmentations and transformations\n",
    "train_transform = T.Compose([\n",
    "    T.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.RandomVerticalFlip(),\n",
    "    T.ColorJitter(\n",
    "        brightness=0.3,\n",
    "        contrast=0.3,\n",
    "        saturation=0.3\n",
    "    ),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "# Define test/validation transformations\n",
    "test_transform = T.Compose([\n",
    "    T.Resize(256),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adf320f",
   "metadata": {},
   "source": [
    "### Build the CoralReefDataset Class\n",
    "We use a Dataset class because it gives PyTorch a consistent way to load individual training examples, apply transforms, and retrieve labels on demand. This keeps the data-loading logic organized and allows the DataLoader to automatically batch, shuffle, and efficiently feed data to the model during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84861cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoralReefDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for MERMAID multi-label classification.\n",
    "    Streams images from S3, applies transforms, and returns label vectors.\n",
    "\n",
    "    CSV format expected:\n",
    "        image_id, region_name, label1, label2, ..., label16\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_path, transform=None):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "\n",
    "        # Drop region_name (col index 1)\n",
    "        if \"region_name\" in self.df.columns:\n",
    "            self.df = self.df.drop(columns=[\"region_name\"])\n",
    "\n",
    "        self.transform = transform\n",
    "        self.image_ids = self.df[\"image_id\"].tolist()\n",
    "\n",
    "        # label columns are everything except image_id\n",
    "        self.label_cols = self.df.columns[1:].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # --- Get image_id ---\n",
    "        image_id = self.image_ids[idx]\n",
    "\n",
    "        # --- Load image from S3 ---\n",
    "        image = get_image_s3(image_id)\n",
    "\n",
    "        # --- Apply transform ---\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # --- Get labels (float32 tensor, shape [16]) ---\n",
    "        labels = torch.tensor(self.df.loc[idx, self.label_cols].values.astype(\"float32\"))\n",
    "\n",
    "        return image, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0c6ff0",
   "metadata": {},
   "source": [
    "### Create Train / Test Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca836159",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"../data/processed/final_labels_train.csv\"\n",
    "test_csv  = \"../data/processed/final_labels_test.csv\"\n",
    "\n",
    "train_dataset = CoralReefDataset(train_csv, transform=train_transform)\n",
    "test_dataset  = CoralReefDataset(test_csv,  transform=test_transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7461bfc4",
   "metadata": {},
   "source": [
    "### Create DataLoaders\n",
    "A DataLoader efficiently feeds batches of data to your model during training by pulling samples from a Dataset, grouping them into mini-batches, and handling details like shuffling and parallel loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ffece5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16, #number of samples per batch\n",
    "    shuffle=True, #prevents memorization of data order\n",
    "    num_workers=0, #parallel data loading\n",
    "    pin_memory=True # speeds up transfer to GPU\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b56f1d",
   "metadata": {},
   "source": [
    "### Build and Configure the ViT Model\n",
    "#### Implement the Model Wrapper\n",
    "A model wrapper is a small class that sits around the pretrained Vision Transformer and replaces its original classification head with a new one, allowing the backbone to stay the same while adapting the model for our specific multi-label coral reef prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df71b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMultiLabel(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer for multi-label classification.\n",
    "    Uses google/vit-base-patch16-224-in21k as the backbone.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_labels=16):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained backbone\n",
    "        self.backbone = ViTModel.from_pretrained(\n",
    "            \"google/vit-base-patch16-224-in21k\"\n",
    "        )\n",
    "\n",
    "        # Hidden size of CLS token representation\n",
    "        hidden_dim = self.backbone.config.hidden_size\n",
    "\n",
    "        # Custom classifier for multi-label prediction\n",
    "        self.classifier = nn.Linear(hidden_dim, num_labels)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        \"\"\"\n",
    "        pixel_values: tensor of shape (B, 3, 224, 224)\n",
    "        \"\"\"\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "\n",
    "        # CLS token is at index 0 of the sequence\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "        logits = self.classifier(cls_embedding)  # shape: (B, num_labels)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c32e0",
   "metadata": {},
   "source": [
    "#### Instantiate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ViTMultiLabel(num_labels=16)\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fafc1",
   "metadata": {},
   "source": [
    "#### Loss Function and Optimizer\n",
    "We use BCEWithLogitsLoss because multi-label classification requires treating each of the 16 benthic attributes as an independent binary prediction, and this loss applies a sigmoid to each output while handling all labels jointly. We use AdamW optimizer because it is the standard, stable optimizer for transformer models and maintains good generalization by decoupling weight decay from the gradient update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3a17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Stage 1 Optimizer\n",
    "optimizer_stage1 = AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()), \n",
    "    lr=5e-4\n",
    ")\n",
    "\n",
    "# Stage 2 Optimizer\n",
    "optimizer_stage2 = AdamW(\n",
    "    model.parameters(), \n",
    "    lr=2e-5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2f40ed",
   "metadata": {},
   "source": [
    "### Stage 1 Training (Backbone Frozen)\n",
    "We freeze the pretrained ViT backbone during Stage 1 so the model can first learn a stable, well-behaved classifier head without disrupting the high-quality visual features already learned from ImageNet-21k, which prevents early overfitting and makes later full fine-tuning more effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6646cbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze Backbone\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf8639f",
   "metadata": {},
   "source": [
    "#### Define Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11817e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_stage1(model, train_loader, criterion, optimizer, device, epochs=5):\n",
    "    model.train() \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Stage 1 training complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba8aa63",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124b0c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_stage1 = 5  \n",
    "train_stage1(\n",
    "    model,\n",
    "    train_loader,\n",
    "    criterion,\n",
    "    optimizer_stage1,\n",
    "    device,\n",
    "    epochs=epochs_stage1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral-reef-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
