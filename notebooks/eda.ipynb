{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c280572c",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8595d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allow imports from the src folder\n",
    "import sys\n",
    "import os\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Create directories for saving processed data and visualizations\n",
    "os.makedirs(\"../data/processed\", exist_ok=True)\n",
    "os.makedirs('../visualizations', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f69591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from src.s3_loader import get_image_s3\n",
    "from src.save_fig import save_fig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce1c147",
   "metadata": {},
   "source": [
    "### Data Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf791a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame's from raw csv data\n",
    "df_features = pd.read_csv('../data/raw/coral_multilabel_dataset.csv')\n",
    "df_annotations = pd.read_csv('../data/raw/metadata_annotations.csv')\n",
    "df_regions = pd.read_csv('../data/raw/metadata_regions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996e2c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect df_features\n",
    "duplicate_count = df_features['image_id'].duplicated().sum()\n",
    "null_count = df_features.isnull().sum().sum()\n",
    "n_rows = len(df_features)\n",
    "print(f\"Duplicate image_ids: {duplicate_count}\")\n",
    "print(f\"Total null values: {null_count}\")\n",
    "print(f'Total rows: {n_rows}')\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4ac088",
   "metadata": {},
   "source": [
    "There are no duplicate images or null values in this dataset. We can see there are 4821 rows (images) and 172 columns (image_id + benthic attributes) in the dataset. The benthic attributes are binary features. We will want to drop image_id for EDA purposes, leaving us with 171 benthic attributes to filter through."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600b6018",
   "metadata": {},
   "source": [
    "### Feature Selection\n",
    "\n",
    "#### Label-Aware Filtering\n",
    "We don't want to overload our CNN's or Vision Transformer with too many benthic attributes to try to learn.  In order to keep the labels to train the model on under ~30, we will select features that appear at least 100 times in the image collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc24235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature columns without 'image_id' \n",
    "feature_cols = df_features.columns.drop(['image_id']) \n",
    "# Label counts for each benthic attribute \n",
    "label_counts = df_features[feature_cols].sum() \n",
    "# Only keep counts with at least 100 positive examples \n",
    "filtered_labels = label_counts[label_counts >= 100].sort_values(ascending=False) \n",
    "# DataFrame of filtered labels + counts print(f'Attributes ≥100 positives: {len(filtered_labels)}') \n",
    "filtered_labels.to_frame(name=\"positive_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d4d07d",
   "metadata": {},
   "source": [
    "We identified 25 benthic attributes with at least 100 positive examples. One of these is the “Other” category. Since “Other” represents everything outside the full set of 171 benthic attributes, it does not provide meaningful ecological information for our reduced feature set. Because of this, we will exclude it from our modeling subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed3e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Other' attribute\n",
    "if 'Other' in filtered_labels.index:\n",
    "    filtered_labels = filtered_labels.drop('Other')\n",
    "\n",
    "print(f'Attributes ≥100 positives (excluding \"Other\"): {len(filtered_labels)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8535788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before vs After Label-Aware Filtering: Side-by-Side Subplots\n",
    "\n",
    "initial_counts = df_features[feature_cols].sum()\n",
    "after_counts = label_counts[filtered_labels.index]\n",
    "\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "# Subplot 1: BEFORE FILTERING\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(initial_counts.values, bins=15, color='steelblue', edgecolor='black')\n",
    "plt.title(\"Label Count Distribution (Before Filtering)\")\n",
    "plt.xlabel(\"Positive Count\")\n",
    "plt.ylabel(\"Number of Labels\")\n",
    "\n",
    "# Subplot 2: AFTER FILTERING\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(after_counts.values, bins=15, color='seagreen', edgecolor='black')\n",
    "plt.title(\"Label Count Distribution (After Filtering)\")\n",
    "plt.xlabel(\"Positive Count\")\n",
    "plt.ylabel(\"Number of Labels\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"label_count_distribution_before_after\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95285a1",
   "metadata": {},
   "source": [
    "#### Region-Aware Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7501e63e",
   "metadata": {},
   "source": [
    "Our goal is to evaluate the model on a geographic region it has never seen during training. For this to work, each region must contain enough examples of the benthic attributes we plan to model. Before deciding which features to keep, we examine the region metadata to understand how these attributes are distributed across regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65ea67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge features with regions so each image has its region_name\n",
    "df_merged = df_features.merge(\n",
    "    df_regions[['image_id', 'region_name']],\n",
    "    on='image_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Compute counts of filtered_labels per region\n",
    "region_label_counts = (\n",
    "    df_merged.groupby('region_name')[filtered_labels.index]\n",
    "    .sum()\n",
    "    .T   # transpose: labels = rows, regions = columns\n",
    ")\n",
    "\n",
    "# Total positives per region across all filtered labels\n",
    "region_positive_counts = region_label_counts.sum(axis=0)\n",
    "\n",
    "# Proportion of all filtered-label positives that come from each region\n",
    "region_positive_props = region_positive_counts / region_positive_counts.sum()\n",
    "\n",
    "print(\"Proportion of filtered-label positives by region:\\n\")\n",
    "print(region_positive_props)\n",
    "\n",
    "region_label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35823b11",
   "metadata": {},
   "source": [
    "Central Indo-Pacific accounts for roughly 55.7 percent of the dataset, Western Indo-Pacific for 43.6 percent, and the Tropical Atlantic for less than 1 percent. Because of this distribution, using Western Indo-Pacific as the test region is reasonable, since the model can train on the larger Central Indo-Pacific subset. The table above also shows that several benthic attributes have very low or no overlap between these regions, so additional feature filtering is be necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb392b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Region Composition Visualization\n",
    "plt.figure(figsize=(6, 4))\n",
    "region_positive_props.sort_values().plot(kind='barh')\n",
    "\n",
    "plt.title(\"Proportion of Filtered-Label Positives by Region\")\n",
    "plt.xlabel(\"Proportion\")\n",
    "plt.ylabel(\"Region\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.4)\n",
    "save_fig(\"region_positive_proportion_barplot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352671d1",
   "metadata": {},
   "source": [
    "We choose a minimum of 100 positive examples per region to ensure each benthic attribute has enough representation for CNNs and Vision Transformers to learn meaningful visual patterns and generalize across geographic regions. It’s acceptable to exclude the Tropical Atlantic because it represents less than one percent of the dataset, so it cannot provide enough examples to support meaningful model training or region-based evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69facf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-region minimum threshold\n",
    "min_per_region = 100  \n",
    "\n",
    "major_regions = ['Central Indo-Pacific', 'Western Indo-Pacific']\n",
    "\n",
    "region_filtered = region_label_counts.loc[\n",
    "    (region_label_counts[major_regions] >= min_per_region).all(axis=1)\n",
    "]\n",
    "\n",
    "# Heatmap of Positive Counts Across Regions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(region_filtered, annot=True, fmt='d', cmap='Blues')\n",
    "\n",
    "plt.title(\"Positive Counts per Benthic Attribute Across Regions\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Benthic Attribute\")\n",
    "save_fig(\"region_filtered_label_heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21edbd4",
   "metadata": {},
   "source": [
    "#### Check for Label Co-Occurrence Structure\n",
    "\n",
    "We check for co-occurrence to ensure that no two benthic attributes always appear together, which would make them redundant and prevent the model from learning distinct visual patterns for each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93433aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation matrix for the final region-filtered labels\n",
    "co_occurrence = df_features[region_filtered.index].corr()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(co_occurrence, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.title(\"Correlation Between Selected Benthic Attributes\")\n",
    "save_fig(\"label_cooccurrence_heatmap\")\n",
    "plt.show()\n",
    "\n",
    "# Count correlations above threshold (excluding diagonal)\n",
    "high_corr_mask = (co_occurrence.abs() > 0.90)\n",
    "high_corr_count = high_corr_mask.sum() - 1   # subtract diagonal\n",
    "\n",
    "# Find high-correlation pairs\n",
    "high_corr_pairs = [\n",
    "    (i, j, co_occurrence.loc[i, j])\n",
    "    for i in co_occurrence.index\n",
    "    for j in co_occurrence.columns\n",
    "    if i < j and abs(co_occurrence.loc[i, j]) > 0.90\n",
    "]\n",
    "\n",
    "# Print result\n",
    "if len(high_corr_pairs) == 0:\n",
    "    print(\"No high correlation pairs found.\")\n",
    "else:\n",
    "    print(\"High correlation pairs (>0.90):\")\n",
    "    for pair in high_corr_pairs:\n",
    "        print(pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62111c0a",
   "metadata": {},
   "source": [
    "#### Per-Region Negative Counts\n",
    "\n",
    "We check per-region negative counts to ensure the model sees both the presence and absence of each benthic attribute, since a label cannot be learned reliably if it never appears as a negative example in a region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72206036",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_neg_counts = df_merged.groupby('region_name')[region_filtered.index] \\\n",
    "                             .apply(lambda x: (x == 0).sum())\n",
    "\n",
    "# Heatmap of Negative Counts Across Regions\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(region_neg_counts, annot=True, fmt='d', cmap='Reds')\n",
    "\n",
    "plt.title(\"Negative Counts per Benthic Attribute Across Regions\")\n",
    "plt.xlabel(\"Region\")\n",
    "plt.ylabel(\"Benthic Attribute\")\n",
    "save_fig(\"negative_counts_heatmap\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d63483",
   "metadata": {},
   "source": [
    "In our dataset, both major regions contain substantial negative counts for all selected attributes, which confirms that each label provides a meaningful learning signal and can be distinguished reliably across regions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf90da7",
   "metadata": {},
   "source": [
    "#### Prevalence Distribution\n",
    "\n",
    "We examine the prevalence of each attribute to detect extremely rare or overly common labels that could cause imbalance during training or skew evaluation metrics. This is especially important for multi-label deep learning because extreme imbalance can cause unstable training, biased gradients and poor recall on rare classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3921e71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate prevalence for each selected label (proportion of images where label==1)\n",
    "prevalence = df_features[region_filtered.index].mean().sort_values(ascending=False)\n",
    "\n",
    "# Convert to percent for easier interpretation\n",
    "prevalence_percent = (prevalence * 100).round(2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=prevalence_percent.values, y=prevalence_percent.index)\n",
    "\n",
    "plt.xlabel(\"Prevalence (% of images)\")\n",
    "plt.ylabel(\"Benthic Attribute\")\n",
    "plt.title(\"Prevalence of Selected Benthic Attributes\")\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.5)\n",
    "save_fig(\"benthic_attribute_prevalence\")\n",
    "plt.show()\n",
    "\n",
    "prevalence_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dc3b68",
   "metadata": {},
   "source": [
    "Most benthic attributes show moderate to high prevalence across the dataset, while even the rarest remaining classes appear in more than five percent of images, suggesting that the final feature set does not suffer from extreme imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68002aba",
   "metadata": {},
   "source": [
    "### Image Sampling and Visual Diagnostics\n",
    "#### Random Image Grid\n",
    "\n",
    "To get a quick visual sense of the overall image quality in the MERMAID dataset, we sample nine images at random and display them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b6865a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x3 grid of randomly selected images from the dataset\n",
    "random.seed(33)\n",
    "sample_ids = random.sample(df_features['image_id'].tolist(), 9)\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for i, img_id in enumerate(sample_ids, 1):\n",
    "    try:\n",
    "        img = get_image_s3(img_id).convert(\"RGB\")\n",
    "        plt.subplot(3, 3, i)\n",
    "        plt.imshow(img)\n",
    "        plt.title(img_id, fontsize=8)\n",
    "        plt.axis(\"off\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {img_id}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"random_image_grid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90a83fd",
   "metadata": {},
   "source": [
    "These samples show clear underwater scenes with good color balance and visible benthic structure, which suggests that the dataset is suitable for image-based learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6527a808",
   "metadata": {},
   "source": [
    "#### Region-Based Image Grid\n",
    "\n",
    "To compare imagery across geographic realms, we sample several images from each major region and place them side by side for visual inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466c8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3x2 grid of region-specific images\n",
    "random.seed(33)\n",
    "regions = [\"Central Indo-Pacific\", \"Western Indo-Pacific\"]\n",
    "images_per_region = 3\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "index = 1\n",
    "for region in regions:\n",
    "    region_ids = df_regions[df_regions[\"region_name\"] == region][\"image_id\"].tolist()\n",
    "    subset = random.sample(region_ids, images_per_region)\n",
    "\n",
    "    for img_id in subset:\n",
    "        try:\n",
    "            img = get_image_s3(img_id).convert(\"RGB\")\n",
    "            plt.subplot(len(regions), images_per_region, index)\n",
    "            plt.imshow(img)\n",
    "            plt.title(f\"{region}\\n{img_id}\", fontsize=7)\n",
    "            plt.axis(\"off\")\n",
    "            index += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_id}: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"region_based_image_grid\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604769fe",
   "metadata": {},
   "source": [
    "Although the images originate from different biogeographic regions, they do not exhibit clear region-specific visual cues. This supports our choice of a region-based train-test split, since the model cannot rely on trivial region signatures and must instead learn benthic features that generalize across locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10640ca7",
   "metadata": {},
   "source": [
    "#### Image Resolution and Aspect Ratio Analysis\n",
    "\n",
    "CNNs and ViTs assume consistent image sizes. MERMAID images are usually uniform, but we want to confirm this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20abf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image_ids \n",
    "random.seed(33)\n",
    "sample_size = 40\n",
    "sample_ids = random.sample(df_features['image_id'].tolist(), sample_size)\n",
    "\n",
    "widths = []\n",
    "heights = []\n",
    "aspect_ratios = []\n",
    "\n",
    "for img_id in sample_ids:\n",
    "    try:\n",
    "        img = get_image_s3(img_id).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        widths.append(w)\n",
    "        heights.append(h)\n",
    "        aspect_ratios.append(w / h)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {img_id}: {e}\")\n",
    "\n",
    "plt.figure(figsize=(18, 5))\n",
    "\n",
    "# Plot width distribution\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(widths, bins=20, color='royalblue', edgecolor='black')\n",
    "plt.title(\"Image Width Distribution\")\n",
    "plt.xlabel(\"Width (pixels)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Plot height distribution\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(heights, bins=20, color='seagreen', edgecolor='black')\n",
    "plt.title(\"Image Height Distribution\")\n",
    "plt.xlabel(\"Height (pixels)\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Plot aspect ratio distribution\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(aspect_ratios, bins=20, color='salmon', edgecolor='black')\n",
    "plt.title(\"Aspect Ratio Distribution\")\n",
    "plt.xlabel(\"Width / Height\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.grid(alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"resolution_and_aspect_ratio_distributions\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcd95eb",
   "metadata": {},
   "source": [
    "The widths and heights fall into several clear high-resolution groups rather than one unified size. This likely reflects different cameras or export settings used in the MERMAID surveys. Even though the resolutions vary, the aspect ratios cluster tightly around a few values near one, which means the images all have a similar overall square shape. Because of this consistency, resizing them to a single standard input size will only introduce mild and uniform distortion, so there is no need for multiple preprocessing pipelines. The dataset is fully compatible with a single global resize step before model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237e6166",
   "metadata": {},
   "source": [
    "#### Pixel Intensity and RGB Distribution\n",
    "\n",
    "Understanding the pixel intensity and RGB channel distributions matters because underwater images often have uneven lighting, strong blue–green color dominance, reduced red wavelengths, and variable contrast, all of which influence how the model perceives benthic features and determine the type of normalization and augmentation needed for stable training. Twenty images are sufficient because each image contains millions of pixels, so even a small sample produces tens of millions of color values, which is enough for the overall RGB and brightness distributions to stabilize and reveal the dataset’s true color characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98af6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image IDs\n",
    "random.seed(33)\n",
    "sample_size = 20\n",
    "sample_ids = random.sample(df_features[\"image_id\"].tolist(), sample_size)\n",
    "\n",
    "all_pixels = []\n",
    "r_vals = []\n",
    "g_vals = []\n",
    "b_vals = []\n",
    "\n",
    "for img_id in sample_ids:\n",
    "    try:\n",
    "        img = get_image_s3(img_id).convert(\"RGB\")\n",
    "        arr = np.array(img).reshape(-1, 3)  # flatten H×W×3 into N×3\n",
    "\n",
    "        all_pixels.append(arr)\n",
    "        r_vals.extend(arr[:, 0])\n",
    "        g_vals.extend(arr[:, 1])\n",
    "        b_vals.extend(arr[:, 2])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {img_id}: {e}\")\n",
    "\n",
    "# Convert to arrays\n",
    "r_vals = np.array(r_vals)\n",
    "g_vals = np.array(g_vals)\n",
    "b_vals = np.array(b_vals)\n",
    "\n",
    "# Plot grayscale pixel intensity distribution\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(np.concatenate([r_vals, g_vals, b_vals]), bins=50, color='gray', edgecolor='black')\n",
    "plt.title(\"Overall Pixel Intensity Distribution\")\n",
    "plt.xlabel(\"Pixel Value (0–255)\")\n",
    "plt.ylabel(\"Count\")\n",
    "save_fig(\"pixel_intensity_distribution\")\n",
    "plt.show()\n",
    "\n",
    "# Plot RGB channel histograms\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(r_vals, bins=50, color='red', edgecolor='black')\n",
    "plt.title(\"Red Channel\")\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(g_vals, bins=50, color='green', edgecolor='black')\n",
    "plt.title(\"Green Channel\")\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(b_vals, bins=50, color='blue', edgecolor='black')\n",
    "plt.title(\"Blue Channel\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"rgb_channel_histograms\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b777bbcb",
   "metadata": {},
   "source": [
    "The overall pixel intensity distribution shows that most pixel values fall between roughly 40 and 160, which indicates moderate brightness without severe underexposure or overexposure. The tail above 200 suggests small regions of strong highlights, which is expected when sunlight reflects off sand or bleaching patches. The RGB channel histograms reveal a strong imbalance between color channels. The red channel is shifted the lowest, peaking around 50–100, while the green and blue channels peak higher and cover a wider range. This is characteristic of underwater imagery because red wavelengths are absorbed quickly with depth, leaving blue and green tones far more dominant in the raw pixel data. These distributions confirm the need for normalization and color-jitter augmentations during training so the model can generalize across varying lighting, turbidity, and depth conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea45f73",
   "metadata": {},
   "source": [
    "#### Region-Based RGB Comparison\n",
    "\n",
    "To check whether the two major geographic regions differ in overall color characteristics, we compute the mean red, green, and blue values for a small sample of images from each region and compare their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9facf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(33)\n",
    "regions = [\"Central Indo-Pacific\", \"Western Indo-Pacific\"]\n",
    "sample_per_region = 50\n",
    "\n",
    "results = []\n",
    "\n",
    "for region in regions:\n",
    "    region_ids = df_regions[df_regions[\"region_name\"] == region][\"image_id\"].tolist()\n",
    "\n",
    "    # avoid sampling empty list\n",
    "    if len(region_ids) == 0:\n",
    "        print(f\"No images found for region: {region}\")\n",
    "        continue\n",
    "\n",
    "    # sample\n",
    "    n = min(sample_per_region, len(region_ids))\n",
    "    subset = random.sample(region_ids, n)\n",
    "\n",
    "    # compute RGB means\n",
    "    for img_id in subset:\n",
    "        try:\n",
    "            img = get_image_s3(img_id).convert(\"RGB\")\n",
    "            arr = np.array(img)\n",
    "\n",
    "            mean_r = arr[:, :, 0].mean()\n",
    "            mean_g = arr[:, :, 1].mean()\n",
    "            mean_b = arr[:, :, 2].mean()\n",
    "\n",
    "            results.append({\n",
    "                \"region\": region,\n",
    "                \"R\": mean_r,\n",
    "                \"G\": mean_g,\n",
    "                \"B\": mean_b\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_id}: {e}\")\n",
    "\n",
    "df_rgb = pd.DataFrame(results)\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(13, 4))\n",
    "\n",
    "channels = [\"R\", \"G\", \"B\"]\n",
    "\n",
    "for ax, ch in zip(axes, channels):\n",
    "    sns.boxplot(\n",
    "        data=df_rgb,\n",
    "        x=\"region\",\n",
    "        y=ch,\n",
    "        ax=ax\n",
    "    )\n",
    "    ax.set_title(f\"{ch} Channel by Region\")\n",
    "    ax.set_xlabel(\"Region\")\n",
    "    ax.set_ylabel(\"Mean Pixel Value\")\n",
    "\n",
    "plt.tight_layout()\n",
    "save_fig(\"rgb_region_boxplots\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187fd695",
   "metadata": {},
   "source": [
    "The RGB boxplots show that Central Indo-Pacific images are generally a bit brighter, especially in the green and blue channels, while Western Indo-Pacific images tend to be darker and more variable. These differences are not extreme but they do confirm that the two regions have slightly different lighting and color conditions. This supports using normalization and basic color jitter in our preprocessing so the model focuses on benthic features rather than on small region-specific lighting differences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa9c0ef",
   "metadata": {},
   "source": [
    "#### Annotation Consistency Check\n",
    "\n",
    "We want to check annotation-level consistency to ensure that the image-level benthic attributes correspond to what is present in the point-level annotation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecological reliability check \n",
    "annotation_counts = df_annotations['benthic_attribute_name'].value_counts()\n",
    "\n",
    "final_labels = region_filtered.index\n",
    "annotation_final = annotation_counts.reindex(final_labels).fillna(0)\n",
    "\n",
    "image_level_counts = label_counts.reindex(final_labels)\n",
    "\n",
    "consistency_df = pd.DataFrame({\n",
    "    \"image_level_positives\": image_level_counts,\n",
    "    \"annotation_points\": annotation_final.astype(int)\n",
    "}).sort_values(by=\"image_level_positives\", ascending=False)\n",
    "\n",
    "print(\"Image-level vs Annotation-level counts:\\n\")\n",
    "print(consistency_df)\n",
    "\n",
    "low_annotation_labels = consistency_df[consistency_df['annotation_points'] < 20]\n",
    "\n",
    "if len(low_annotation_labels) == 0:\n",
    "    print(\"\\nNo annotation inconsistencies detected (annotation_points ≥ 20).\")\n",
    "else:\n",
    "    print(\"\\nPotential annotation inconsistencies (annotation_points < 20):\")\n",
    "    display(low_annotation_labels)\n",
    "\n",
    "\n",
    "# Logical consistency check (annotation ≥ image)\n",
    "consistency_df[\"annotation_ge_image\"] = (\n",
    "    consistency_df[\"annotation_points\"] >= consistency_df[\"image_level_positives\"]\n",
    ")\n",
    "\n",
    "if consistency_df[\"annotation_ge_image\"].all():\n",
    "    print(\"\\nAll labels are logically consistent: annotation_points >= image_level_positives.\")\n",
    "else:\n",
    "    print(\"\\nLogical inconsistency detected. These labels violate annotation >= image rule:\")\n",
    "    display(consistency_df[~consistency_df[\"annotation_ge_image\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation vs Image-Level Comparison\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "plt.scatter(\n",
    "    consistency_df[\"image_level_positives\"],\n",
    "    consistency_df[\"annotation_points\"],\n",
    "    s=60\n",
    ")\n",
    "\n",
    "for label in consistency_df.index:\n",
    "    x = consistency_df.loc[label, \"image_level_positives\"]\n",
    "    y = consistency_df.loc[label, \"annotation_points\"]\n",
    "    plt.text(x, y, label, fontsize=8)\n",
    "\n",
    "plt.xlabel(\"Image-Level Positives\")\n",
    "plt.ylabel(\"Annotation Points\")\n",
    "plt.title(\"Annotation Points vs Image-Level Positives\")\n",
    "plt.grid(alpha=0.4)\n",
    "save_fig(\"image_vs_annotation_point_scatter\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7766a",
   "metadata": {},
   "source": [
    "This scatter plot compares the number of image-level positives to the number of point-level annotations for each selected attribute, and the upward-right trend shows that attributes with more positive images also have more supporting annotation points, indicating strong and consistent ecological labeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f32fd78",
   "metadata": {},
   "source": [
    "#### Annotated Image \n",
    "\n",
    "To visualize how the point-level labels align with the raw images, we overlay the MERMAID benthic annotations directly on each photo, showing both the grid structure and the specific benthic attribute at every sampled point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98e7a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a few random images that appear in the annotation table\n",
    "random.seed(34)\n",
    "sample_ids = random.sample(\n",
    "    df_annotations[\"image_id\"].unique().tolist(),\n",
    "    3\n",
    ")\n",
    "\n",
    "for img_id in sample_ids:\n",
    "    # load image\n",
    "    img = get_image_s3(img_id).convert(\"RGB\")\n",
    "    arr = np.array(img)\n",
    "\n",
    "    # get annotations for this image\n",
    "    ann = df_annotations[df_annotations[\"image_id\"] == img_id]\n",
    "\n",
    "    plt.figure(figsize=(9, 9))\n",
    "    plt.imshow(arr)\n",
    "    \n",
    "    # plot each point + text\n",
    "    for _, row in ann.iterrows():\n",
    "        x = row[\"col\"]\n",
    "        y = row[\"row\"]\n",
    "        label = row[\"benthic_attribute_name\"]\n",
    "        \n",
    "        # yellow dot\n",
    "        plt.scatter(x, y, c=\"yellow\", s=40, edgecolors=\"black\")\n",
    "        \n",
    "        # text next to the point\n",
    "        plt.text(\n",
    "            x + 5, y - 5, \n",
    "            label, \n",
    "            color=\"white\", \n",
    "            fontsize=7,\n",
    "            bbox=dict(facecolor=\"black\", alpha=0.5, pad=1)\n",
    "        )\n",
    "\n",
    "    plt.title(f\"Annotated Benthic Labels for Image {img_id}\")\n",
    "    plt.axis(\"off\")\n",
    "    save_fig(f\"annotated_image_{img_id}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8995ea0",
   "metadata": {},
   "source": [
    "These examples confirm that the point-level annotations are dense, well-distributed, and accurately linked to their images, providing a strong ecological signal that supports our multi-label training labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128f54f0",
   "metadata": {},
   "source": [
    "### Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc700934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final labels list \n",
    "final_label_cols = [\"image_id\"] + list(final_labels)\n",
    "\n",
    "# Build a dataframe that contains image_id + region_name + selected labels\n",
    "final_df = df_merged[[\"image_id\", \"region_name\"] + list(final_labels)]\n",
    "\n",
    "# Define regions\n",
    "test_region = \"Western Indo-Pacific\"\n",
    "train_regions = [\"Central Indo-Pacific\", \"Tropical Atlantic\"]\n",
    "\n",
    "# Create datasets\n",
    "df_full = df_features[final_label_cols]\n",
    "df_train = final_df[final_df[\"region_name\"].isin(train_regions)].reset_index(drop=True)\n",
    "df_test = final_df[final_df[\"region_name\"] == test_region].reset_index(drop=True)\n",
    "\n",
    "# Save datasets\n",
    "df_full.to_csv(\"../data/processed/final_labels_full.csv\",index=False)\n",
    "df_train.to_csv(\"../data/processed/final_labels_train.csv\", index=False)\n",
    "df_test.to_csv(\"../data/processed/final_labels_test.csv\", index=False)\n",
    "\n",
    "print(\"Processed EDA outputs saved to ../data/processed/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coral-reef-classification",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
